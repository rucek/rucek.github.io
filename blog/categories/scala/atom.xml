<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Scala | Jacek Kunicki]]></title>
  <link href="http://rucek.github.io/blog/categories/scala/atom.xml" rel="self"/>
  <link href="http://rucek.github.io/"/>
  <updated>2017-04-18T11:55:59+02:00</updated>
  <id>http://rucek.github.io/</id>
  <author>
    <name><![CDATA[Jacek Kunicki]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Benchmarking Single-result Query Performance in Slick Using JMH]]></title>
    <link href="http://rucek.github.io/blog/2017/04/12/benchmarking-single-result-query-performance-in-slick-using-jmh/"/>
    <updated>2017-04-12T14:41:12+02:00</updated>
    <id>http://rucek.github.io/blog/2017/04/12/benchmarking-single-result-query-performance-in-slick-using-jmh</id>
    <content type="html"><![CDATA[<h2>Background</h2>

<p><a href="http://slick.lightbend.com/">Slick</a> offers a typesafe DSL for accessing your database tables as if they were Scala collections, with a similar API. However, since at the very end the DSL is translated into SQL queries, you sometimes need to be very careful how you use it. In this post you&rsquo;re going to see how a subtle difference in the query DSL can affect the performance significantly.</p>

<h2>Test case</h2>

<p>To illustrate the performance differences, we&rsquo;re going to fetch a single row from a large table defined as follows:</p>

<pre><code class="scala">case class User(id: Int, name: String)

class Users(tag: Tag) extends Table[User](tag, "users") {

  def id = column[Int]("id")

  def name = column[String]("name")

  def * = (id, name) &lt;&gt; (User.tupled, User.unapply)
}

val users = TableQuery[Users]
</code></pre>

<p>Now consider the two following queries:</p>

<pre><code class="scala">val q1 = users.result.head
val q2 = users.take(1).result.head
</code></pre>

<p>Can you tell which of those is going to be faster and why? It turns out that <code>q1</code> is going to fetch all the rows from the table and only then take the first one, while <code>q2</code> is going to add a <code>LIMIT 1</code> clause to the generated SQL query, thus reducing the number of rows fetched to one. The full generated SQL queries are, respectively:</p>

<pre><code class="sql">select "id", "name" from "users" -- q1
select "id", "name" from "users" limit 1 -- q2
</code></pre>

<p>So it&rsquo;s rather obvious that <code>q2</code> is going to be much more efficient. But let&rsquo;s check to make sure.</p>

<h2>Benchmarks</h2>

<p>To check the actual performance differences, I used <a href="http://openjdk.java.net/projects/code-tools/jmh/">JMH</a> - the Java micro benchmarking tool - and the <a href="https://github.com/ktoso/sbt-jmh">sbt-jmh plugin</a> (kudos to <a href="http://aludwikowski.blogspot.com/">Andrzej Ludwikowski</a> for recommending it!). JMH basically lets you test any part of your existing code, e.g. a single method - hence the <em>micro</em> in the name. It takes care of warming up the JVM, computing statistics etc. - the entire boilerplate you would like to skip. The benchmarks can either be configured using command-line parameters or with annotations - let&rsquo;s see how to use the latter approach.</p>

<h3>Benchmark parameters</h3>

<p>Firstly, you want to specify two types of queries to benchmark, called <code>take</code>, for the limited variant, and <code>head</code> for the second one. Secondly, you&rsquo;d like to check how the queries behave for different numbers of records in the table - let&rsquo;s assume 10k, 50k, 100k and 500k. The initial banchmark code looks like:</p>

<pre><code class="scala">@State(Scope.Benchmark)
class SingleResultBenchmark {

  @Param(Array("take", "head"))
  var queryType: String = ""

  @Param(Array("10000", "50000", "100000", "500000"))
  var numberOfRecords: Int = 0
}
</code></pre>

<p>This is certainly not the best Scala code you could imagine, but this is due to the way JMH works - the benchmarking Java code is generated by the compiler plugin based on our code, i.e. our code is instrumented by JMH. The resulting limitations are:</p>

<ul>
<li>you need to use <code>var</code>s for the parameters, so that the fields don&rsquo;t become <code>final</code> in the Java code,</li>
<li>you need to initialize the fields with some dummy values, which are not going to be used anyway (unfortunately an abstract class won&rsquo;t work),</li>
<li>all the parameters need to be specified as strings, even for numeric parameters like the <code>numberOfRecords</code>.</li>
</ul>


<p>In order to be able to declare fields (i.e. introduce some internal state) on the benchmarked class, you need to annotate it with <code>@State</code> with the scope of your choice - in our case <code>Scope.Benchmark</code> indicates that the state is going to be shared across all the threads within a single benchmark.</p>

<p>When using multiple benchmark parameters, the benchmarks are going to be executed for every possible combination of those, so in our case there are going to be 8 different benchmarks.</p>

<h3>Initialization</h3>

<p>Before running the actual benchmark, we&rsquo;d like to initialize the database (an embedded H2) with the number of records specified by the <code>numberOfRecords</code> parameter. To easily generate an arbitrary number of instances of the <code>User</code> case class, let&rsquo;s use a nice <a href="https://github.com/DanielaSfregola/random-data-generator">random-data-generator</a> library, which leverages <a href="https://github.com/rickynils/scalacheck">ScalaCheck</a> and <a href="https://github.com/alexarchambault/scalacheck-shapeless">scalacheck-shapeless</a> to provide a simple random generation API. The setup part of the benchmarking code looks like:</p>

<pre><code class="scala">private val db = Database.forConfig("h2")
private val users = TableQuery[Users]

@Setup
def prepare(): Unit = {
  val result = for {
    schemaExists &lt;- db.run(MTable.getTables(Users.TableName).headOption.map(_.nonEmpty))
    _ &lt;- if (schemaExists) Future.successful() else db.run(users.schema.create)
    _ &lt;- db.run(users.delete)
    _ &lt;- db.run(users ++= random[User](numberOfRecords))
  } yield ()

  Await.ready(result, Duration.Inf)
}
</code></pre>

<p>The <code>prepare()</code> method, annotated with <code>@Setup</code> checks if the database schema exists, creates it if it doesn&rsquo;t, then clears the <code>users</code> table and fills it with an arbitrary number of <code>User</code>s.</p>

<h3>The code under test</h3>

<p>This is the most straightforward part, since you just pick one of the queries based on the <code>queryName</code> parameter and execute it:</p>

<pre><code class="scala">private val queries = Map(
  "take" -&gt; users.take(1).result.head,
  "head" -&gt; users.result.head
)

@Benchmark
@BenchmarkMode(Array(Mode.AverageTime))
def query(): Unit = Await.ready(db.run(queries(queryType)), Duration.Inf)
</code></pre>

<p>Here, the <code>query()</code> method is annotated with <code>@Benchmark</code> to indicate that this is the actual part of the application that you want to benchmark, and with <code>BenchmarkMode</code> to indicate that you want to measure the execution time (the default mode is to measure the throughput).</p>

<h3>Results</h3>

<p>Now you&rsquo;re ready to run the benchmarks, which is as simple as:</p>

<pre><code class="``bash">  sbt jmh:run
</code></pre>

<p>Please note that with the default parameters there will be 10 forks for every parameter combination, each fork consisting of 20 warm-up iterations and 20 actual ones, which leads to quite a number of executions in total and takes some time to complete. To run fewer forks and thus reduce the time, you can e.g. add a command-line parameter:</p>

<pre><code class="``bash">  sbt "jmh:run -f 1"
</code></pre>

<p>which reduces the number of forks to one.</p>

<p>Below are the results of running the (lengthy) benchmark with the default parameters on a MacBook with 2,3 GHz Intel Core i7 and 16 GB of RAM under macOS 10.2.3 (Sierra):</p>

<pre><code class="``">  (numberOfRecords)  (queryType)  Mode  Cnt  Score    Error  Units
              10000         take  avgt  200  0.001 ±  0.001   s/op
              10000         head  avgt  200  0.008 ±  0.001   s/op
              50000         take  avgt  200  0.001 ±  0.001   s/op
              50000         head  avgt  200  0.035 ±  0.003   s/op
             100000         take  avgt  200  0.001 ±  0.001   s/op
             100000         head  avgt  200  0.064 ±  0.005   s/op
             500000         take  avgt  200  0.001 ±  0.001   s/op
             500000         head  avgt  200  3.571 ±  0.168   s/op
</code></pre>

<p>They certainly prove that the query with <code>take(1)</code> is much faster - you could even say that its performance is constant with the growing number of records, while the non-limited query tends to get slower when the record count increases (which is expected).</p>

<h2>Summary</h2>

<p>Although the Slick API resembles Scala collections a lot, you always need to remember that there&rsquo;s a database underneath and thus wisely choose how you use the API. In case you&rsquo;re not sure which of several approaches is faster, you can have a look at the actual SQL generated by Slick. When just looking at the queries is not sufficient, you can use a micro benchamrking tool like JMH to verify your guesses.</p>

<p>A full working example with the code presented above is available <a href="https://github.com/rucek/slick-single-result-performance">on GitHub</a>. Enjoy!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Implementing a Custom Akka Streams Graph Stage]]></title>
    <link href="http://rucek.github.io/blog/2016/07/20/implementing-a-custom-akka-streams-graph-stage/"/>
    <updated>2016-07-20T11:39:00+02:00</updated>
    <id>http://rucek.github.io/blog/2016/07/20/implementing-a-custom-akka-streams-graph-stage</id>
    <content type="html"><![CDATA[<h2>Background</h2>

<p><a href="http://doc.akka.io/docs/akka/2.4.8/scala/stream/index.html">Akka Streams</a> offers a number of predefined building blocks for your graphs (i.e. processing pipelines). Should you need a non-standard solution, there&rsquo;s an API to help you write the custom part of the graph. In this post I&rsquo;m going to walk you through implementing your own graph stage.</p>

<h2>Recap: Akka Streams concepts</h2>

<p>Since the stream processing terminology heavily depends on the library/toolkit you are using, here is a quick reminder of what things are called in the Akka Streams world: the producer is called a <code>Source</code>, the consumer - a <code>Sink</code> and the processing stages are <code>Flow</code>s. Each of those is a specialized graph stage whose type is determined by the number of inputs and outputs - a <code>Source</code> has no inputs and a single output, a <code>Sink</code> has a single input and no outputs, a <code>Flow</code> has a single input and a single output.</p>

<p>In terms of the types, each part of the graph is a <code>GraphStage</code> with a given <code>Shape</code> - with the most basic shapes being: <code>SourceShape</code>, <code>FlowShape</code> and <code>SinkShape</code>. There are also other more complex <code>Shape</code>s available, used for modelling such concepts as broadcasting or merging elements of the stream, but those are out of the scope of this post.</p>

<h2>The use case</h2>

<p>Let&rsquo;s say that having a stream of elements of type <code>E</code> you want to observe their arbitrary property of type <code>P</code>, accumulate the elements as long as the property remains unchanged and only emit an <code>immutable.Seq[E]</code> of accumulated elements when the property changes. In a real-life example the elements can be e.g. lines in a CSV file which you would like to group by a given field.</p>

<h2>Anatomy of a custom graph stage</h2>

<p>A custom graph stage is nothing more than an implementation of:</p>

<pre><code class="scala">abstract class GraphStage[S &lt;: Shape]
</code></pre>

<p>In our example the stage is going to have a single input and a single output, which makes it a <code>Flow</code> whose shape is:</p>

<pre><code class="scala">FlowShape[E, immutable.Seq[E]]
</code></pre>

<p>The definition of the stage thus becomes:</p>

<pre><code class="scala">final class AccumulateWhileUnchanged[E] extends GraphStage[FlowShape[E, immutable.Seq[E]]] {
  // ...
}
</code></pre>

<p>Now you just need to implement two methods</p>

<ul>
<li><code>def shape: FlowShape</code> - to provide a concrete shape</li>
<li><code>def createLogic(inheritedAttributes: Attributes): GraphStageLogic</code> - to provide your custom logic of the stage</li>
</ul>


<p>Let&rsquo;s now dig into the details of those two methods.</p>

<h2>Implementing a custom graph stage</h2>

<h3>Providing a custom <code>FlowShape</code></h3>

<p>A <code>FlowShape</code> simply consists of an <code>Inlet</code> and an <code>Outlet</code>, i.e. the <em>ports</em> of the stage. To define a port, you need to provide its name and data type. After defining the ports, the stage implementation becomes:</p>

<pre><code class="scala">final class AccumulateWhileUnchanged[E] extends GraphStage[FlowShape[E, immutable.Seq[E]]] {

  val in = Inlet[E]("AccumulateWhileUnchanged.in")
  val out = Outlet[immutable.Seq[E]]("AccumulateWhileUnchanged.out")

  override def shape = FlowShape(in, out)
}
</code></pre>

<h3>Providing a custom <code>GraphStageLogic</code></h3>

<p>Since the <code>GraphStage</code>s are meant to be reusable, it is crucial to keep them immutable, i.e. not to put any mutable state inside them. On the other hand, however, the stage we are implementing here is definitely stateful - its state consists of the accumulated elements. Here is where the <code>GraphStageLogic</code> comes to the rescue - since a new instance of it is created for every materialization of the flow, it is the one and only place to keep the mutable state in.</p>

<p>Within the <code>GraphStageLogic</code>, apart from keeping the mutable state, you may also define handlers for the <code>onPush()</code> and <code>onPull()</code> events. The <code>onPush()</code> event occurs when a new element from the upstream is available and can be acquired using <code>grab()</code>. The <code>onPull()</code>, on the other hand, occurs when the downstream is ready to accept a new element which can be sent with <code>push()</code>.</p>

<p>So here is what a draft implementation of the <code>GraphStageLogic</code> with the handlers is going to look like:</p>

<pre><code class="scala">override def createLogic(inheritedAttributes: Attributes) = new GraphStageLogic(shape) {

  setHandlers(in, out, new InHandler with OutHandler {

    override def onPush(): Unit = {
      // ...
    }

    override def onPull(): Unit = {
      // ...
    }
  })
}
</code></pre>

<p>To implement the actual accumulating logic, you need to:</p>

<ul>
<li>know how to extract the observed property of the incoming elements,</li>
<li>keep track of the incoming elements in some kind of a buffer.</li>
</ul>


<h4>Extracting the observed property</h4>

<p>The easiest way to know which property to observe is to have the user provide a function which extracts this property - so you need to adjust the stage definition a bit:</p>

<pre><code class="scala">final class AccumulateWhileUnchanged[E, P](propertyExtractor: E =&gt; P)
  extends GraphStage[FlowShape[E, immutable.Seq[E]]] {
</code></pre>

<h4>Keeping track of the incoming elements</h4>

<p>The internal state of your stage logic will consist of:</p>

<ul>
<li>an <code>Option[P]</code> to keep the current value of the observed property (empty until the first element arrives),</li>
<li>a <code>Vector[E]</code> to accumulate the elements (cleared when the observed property changes).</li>
</ul>


<p>When the next input element arrives (in <code>onPush()</code>), you want to extract its property and check if it differs from the current value. If there is no current value yet or the values are equal, you add the element to the buffer and <code>pull()</code> the input, otherwise you <code>push()</code> the buffer contents downstream and clear the buffer. When the downstream requests a new sequence of elements with <code>onPull()</code>, you just need to <code>pull()</code> the input in order to indicate, that the stage is ready to accept a new incoming element.</p>

<p>An additional case that you need to handle is when the upstream has completed (i.e. no more input elements are going to arrive or there was an error in the upstream) - then you need to push the <em>last</em> elements from the buffer (unless it is empty) and complete the stage afterwards. Moreover, to be nice to memory and the GC, you may wish to clear the buffer after the stage is complete.</p>

<p>The full implementation of the above concepts is going to be something like:</p>

<pre><code class="scala">final class AccumulateWhileUnchanged[E, P](propertyExtractor: E =&gt; P)
  extends GraphStage[FlowShape[E, immutable.Seq[E]]] {

  val in = Inlet[E]("AccumulateWhileUnchanged.in")
  val out = Outlet[immutable.Seq[E]]("AccumulateWhileUnchanged.out")

  override def shape = FlowShape.of(in, out)

  override def createLogic(inheritedAttributes: Attributes) = new GraphStageLogic(shape) {

    private var currentState: Option[P] = None
    private val buffer = Vector.newBuilder[E]

    setHandlers(in, out, new InHandler with OutHandler {

      override def onPush(): Unit = {
        val nextElement = grab(in)
        val nextState = propertyExtractor(nextElement)

        if (currentState.isEmpty || currentState.contains(nextState)) {
          buffer += nextElement
          pull(in)
        } else {
          val result = buffer.result()
          buffer.clear()
          buffer += nextElement
          push(out, result)
        }

        currentState = Some(nextState)
      }

      override def onPull(): Unit = {
        pull(in)
      }

      override def onUpstreamFinish(): Unit = {
        val result = buffer.result()
        if (result.nonEmpty) {
          emit(out, result)
        }
        completeStage()
      }
    })

    override def postStop(): Unit = {
      buffer.clear()
    }
  }
}
</code></pre>

<p>If you are wondering why <code>emit()</code> is used instead of <code>push()</code> in <code>onUsptreamFinish()</code> (line 40), the answer is - because it is not possible to push a port which has not been pulled. Once the upstream is finished, the buffer may still contain the final group of accumulated elements - but chances are that the output port has not been pulled after the previous group was pushed. You want, however, to send the final group anyway - that is where <code>emit()</code> comes to the rescue - when it detects that the output port is not available (i.e. cannot be pushed), it replaces the <code>OutHandler</code> with a temporary one and only then does it execute the actual <code>push()</code>.</p>

<p>Now you are ready to use the custom stage in your application with <code>.via(new AccumulateWhileUnchanged(...))</code>. For example, having a simple domain like:</p>

<pre><code class="scala">case class Element(id: Int, value: Int)

object SampleElements {

  val E11 = Element(1, 1)
  val E21 = Element(2, 1)
  val E31 = Element(3, 1)
  val E42 = Element(4, 2)
  val E52 = Element(5, 2)
  val E63 = Element(6, 3)

  val Ones = immutable.Seq(E11, E21, E31)
  val Twos = immutable.Seq(E42, E52)
  val Threes = immutable.Seq(E63)

  val All = Ones ++ Twos ++ Threes
}
</code></pre>

<p>when you run:</p>

<pre><code class="scala">Source(SampleElements.All)
  .via(new AccumulateWhileUnchanged(_.value))
  .runWith(Sink.foreach(println))
</code></pre>

<p>the output will be:</p>

<pre><code>Vector(Element(1,1), Element(2,1), Element(3,1))
Vector(Element(4,2), Element(5,2))
Vector(Element(6,3))
</code></pre>

<h2>Testing</h2>

<p>There is a number of useful utilities to help you test your custom graph stages. With the help of those and using the <code>SampleElements</code> helper defined above, a sample test case for the above stage looks like:</p>

<pre><code class="scala">"AccumulateWhileUnchanged" should {

  "emit accumulated elements when the given property changes" in {
    val (_, sink) = Source(SampleElements.All)
      .via(AccumulateWhileUnchanged(_.value))
      .toMat(TestSink.probe)(Keep.both)
      .run()

    sink.request(42)
    sink.expectNext(SampleElements.Ones, SampleElements.Twos, SampleElements.Threes)
    sink.expectComplete()
  }
}
</code></pre>

<p>The <code>TestSink.probe</code> (line 6) creates an instance of <code>akka.stream.testkit.TestSubscriber.Probe</code>, which offers methods such as <code>expectNext()</code> or <code>expectComplete()</code> (lines 10-11) to verify whether the stage behaves correctly.</p>

<h2>Summary</h2>

<p>After diligently going through this post, you should understand how the <code>GraphStage</code> API is designed and how to use it to implement your own graph stage.</p>

<p>For even more details, please refer to the <a href="http://doc.akka.io/docs/akka/2.4.8/scala/stream/stream-customize.html">Custom stream processing</a> section of the Akka Streams documentation.</p>

<p>If you find the <code>AccumulateWhileUnchanged</code> stage useful, there is no need to rewrite it from scratch, since it is a part of <a href="https://github.com/akka/akka-stream-contrib">akka-stream-contrib</a> - a project which groups various add-ons to Akka Streams core.</p>
]]></content>
  </entry>
  
</feed>
